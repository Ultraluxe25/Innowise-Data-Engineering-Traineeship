<h1 align="center">Types of files</h1>


## Description

In this task you will see more information about different types of files.
Also this topic will be mixed in languages(Russian+English)

## Parquet

Parquet является довольно сложным форматом по сравнению с тем же текстовым файлом с json внутри.
Примечательно, что свои корни этот формат пустил даже в разработки Google, а именно в их проект под названием Dremel — об этом уже упоминалось на Хабре, но мы не будем углубляться 
в дебри Dremel, желающие могут прочитать об этом тут: research.google.com/pubs/pub36632.html.

Если коротко, Parquet использует архитектуру, основанную на “уровнях определения” (definition levels) и “уровнях повторения” (repetition levels), что позволяет довольно 
эффективно кодировать данные, а информация о схеме выносится в отдельные метаданные.
При этом оптимально хранятся и пустые значения.

Структура Parquet-файла хорошо проиллюстрирована в документации:

<p align="center">
<img src="https://habrastorage.org/files/00c/814/4e6/00c8144e68f14eb388f8636717a7667a.gif" width="80%"></p>

Файлы имеют несколько уровней разбиения на части, благодаря чему возможно довольно эффективное параллельное исполнение операций поверх них:

Row-group — это разбиение, позволяющее параллельно работать с данными на уровне Map-Reduce

Column chunk — разбиение на уровне колонок, позволяющее распределять IO операции

Page — Разбиение колонок на страницы, позволяющее распределять работу по кодированию и сжатию

Если сохранить данные в parquet файл на диск, используя самою привычную нам файловую систему, вы обнаружите, что вместо файла создаётся директория, в которой содержится целая коллекция файлов. 
Часть из них — это метаинформация, в ней — схема, а также различная служебная информация, включая частичный индекс, позволяющий считывать только необходимые блоки данных при запросе. 
Остальные части, или партиции, это и есть наши Row group.

Для интуитивного понимания будем считать Row groups набором файлов, объединённых общей информацией. Кстати, это разбиение используется HDFS для реализации data locality, когда каждая нода в 
кластере может считывать те данные, которые непосредственно расположены у неё на диске. Более того, row group выступает единицей Map Reduce, и каждая map-reduce задача в Spark работает со 
своей row-group. Поэтому worker обязан поместить группу строк в память, и при настройке размера группы надо учитывать минимальный объём памяти, выделяемый на задачу на самой слабой ноде, 
иначе можно наткнуться на OOM.

Column chunk (разбиение на уровне колонок) — оптимизирует работу с диском (дисками). Если представить данные как таблицу, то они записываются не построчно, а по колонкам.

Представим таблицу:

<p align="center">
<img src="https://habrastorage.org/r/w1560/files/469/897/dae/469897dae11f4b159afd1f2bef6d5d6d.png" width="50%"></p>

Тогда в текстовом файле, скажем, csv мы бы хранили данные на диске примерно так:

<p align="center">
<img src="https://habrastorage.org/files/b4f/916/6d3/b4f9166d324a41f395406d52feec5d7b.png" width="80%"></p>

В случае с Parquet:

<p align="center">
<img src="https://habrastorage.org/files/b45/9aa/ce8/b459aace83f5497aa7dbd37a9e50b493.png" width="80%"></p>

Благодаря этому мы можем считывать только необходимые нам колонки.

Из всего многообразия колонок на деле аналитику в конкретный момент нужны лишь несколько, к тому же большинство колонок остается пустыми. Parquet в разы ускоряет процесс работы с данными,
более того — подобное структурирование информации упрощает сжатие и кодирование данных за счёт их однородности и похожести.
Каждая колонка делится на страницы (Pages), которые, в свою очередь, содержат метаинформацию и данные, закодированные по принципу архитектуры из проекта Dremel. 
За счёт этого достигается довольно эффективное и быстрое кодирование. Кроме того, на данном уровне производится сжатие (если оно настроено). На данный момент доступны кодеки snappy, gzip, lzo.

Есть ли подводные камни?

За счёт “паркетной” организации данных сложно настроить их стриминг — если передавать данные, то полностью всё группу. Также, если вы утеряли метаинформацию или изменили контрольную 
сумму для cтраницы данных, то вся страница будет потеряна (если для Column chank — то chank потерян, аналогично для row group). На каждом из уровней разбиения строятся контрольные суммы, 
так что можно отключить их вычисления на уровне файловой системы для улучшения производительности.

Выводы:

Достоинства хранения данных в Parquet:

- Несмотря на то, что они и созданы для hdfs, данные могут храниться и в других файловых системах, таких как GlusterFs или поверх NFS
- По сути это просто файлы, а значит с ними легко работать, перемещать, бэкапить и реплицировать.
- Колончатый вид позволяет значительно ускорить работу аналитика, если ему не нужны все колонки сразу.
- Нативная поддержка в Spark из коробки обеспечивает возможность просто взять и сохранить файл в любимое хранилище.
- Эффективное хранение с точки зрения занимаемого места.
- Как показывает практика, именно этот способ обеспечивает самую быструю работу на чтение по сравнению с использованием других файловых форматов.


Недостатки:

- Колончатый вид заставляет задумываться о схеме и типах данных.
- Кроме как в Spark, Parquet не всегда обладает нативной поддержкой в других продуктах.
- Не поддерживает изменение данных и эволюцию схемы. Конечно, Spark умеет мерджить схему, если у вас она меняется со временем (для этого надо указать специальную опцию при чтении), но, 
чтобы что-то изменить в уже существующим файле, нельзя обойтись без перезаписи, разве что можно добавить новую колонку.
- Не поддерживаются транзакции, так как это обычные файлы а не БД.

Note: статья в которой объясняется другими словами(https://www.bigdataschool.ru/wiki/parquet?ysclid=l87biclda9504593918)

От меня краткая сводка(если буду спрашивать на собесе, то что-то из этого): 

- Во-первых parquet оптимизирован для работы со Spark
- Parquet отлично сжимает данные(похуже ORC, но на втором месте) и я продемонстрирую это чуть позже в это уроке
- Существует такая крутая вещь как Delta Lake, я затрону эту тему подробнее в более поздних уроках, а пока вам просто нужно знать что Delta Lake работает с Parquet файлами
- Parquet хранит данные по колонкам, а это значит что если вам надо только x колонок, то вы считаете только x колонок, в отличие например от csv где придётся считать все колонки
- С топика PySpark_Basics вы уже должны знать, что Parquet хранит мета-информацию, тем самым Spark не надо считывать все строки как в csv для забора типа колонок и прочих штук, он это 
заберёт из мета-информации
- Несмотря на то что в официальной документации написано что snappy кодирование эффектит на то, что файл можно считать будет считать только целиком(нельзя считать частично), 
сжатие производится как раз на уровне pages, поэтому из самого parquet файла всё ещё можно считывать только то что нужно

## ORC

Я нашел только один нормальный источник где только про этот формат, а это значит что вот ссылка(https://www.bigdataschool.ru/wiki/orc?ysclid=l87d66i1a4726280864).

От меня краткая сводка:

- Сжимает лучше чем Parquet при чём солидно
- Хранит данные по колонкам, а значит снова читаем только то что нам нужно
- Хранит мета-информацию, а значит всё берется из него и не надо обходить весь файл

И тут вопрос, а зачем тогда вообще Parquet, если ORC такой же так ещё и сжимает лучше? Ответ в этой статье(https://medium.com/@dhareshwarganesh/benchmarking-parquet-vs-orc-d52c39849aef).

От себя могу сказать что Parquet изначально был для Spark, в то время как ORC для Hive(Spark умел читать вектора из Spark, ну а Hive из ORC). Сейчас же и то и то умеет читать
вектора и оттуда и оттуда, поэтому различаются они следующим:

- Parquet если ваши данные вложенные, ORC для плоских данных
- Нужно больше сжатие? Тогда вам ORC. Нужна скорость(зачастую как раз она и нужна)? Тогда вам Parquet
- Да Parquet не поддерживает ACID в отличие от ORC, но тут приходит Delta Lake который поддерживает и работает как раз с Parquet.
- В 99% будет Parquet ибо с ним быстрее работать

## CSV

Факты:

- Хранит данные по строкам(это значит что всегда все колонки считываются)
- Нет сжатия(P.s. можно сжать через архиватор, например gz)
- Нет мета-данных

## JSON

До сих пор все рассматриваемые файлы были splittable: то есть в рамках Spark у нас есть много потоков готовых параллельно считывать данные. Если файл splittable, то
тогда его будут считывать сразу несколько потоков. JSON же не является splittable, а значит он всегда считывается целиком, что может стать огромной проблемой(например JSON размером 1гб
пойдёт в одну партицию т.к. его нельзя разделить, что может привести к spill-эффекту или ещё хуже OOM, подробнее об этом в следующих уроках). Ну а вообще JSON по сути ключ-значение.
P.s. Вообще есть разработка(ток не помню от кого) расширения для JSON, который является splittable, но думаю вы его не встретите.

## Avro

Статейка(https://www.bigdataschool.ru/blog/kafka-big-data-apache-avro.html?ysclid=l87f4mn9lc361070957).

Особо не знаю что сказать, кроме как что для него нужна особая Avro-схема без которой жить не может. Лучше чем JSON. Сам с ним не сталкивался, так что сразу перейду к следующему.

## Разница между Avro, Parquet и ORC

Прикреплю статью, чтобы ещё раз пройтись по основным типам файлов Big Data, плюс в конце в нём есть сравнение(https://habr.com/ru/company/vk/blog/504952/?ysclid=l87ddj9j44827638583).
 
